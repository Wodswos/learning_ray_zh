{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1471c172-8706-4c9b-9e81-40c4b95c1d0f",
   "metadata": {},
   "source": [
    "Ray Serve 主要会有三类实例，他们都是基于 Ray Core 创建的 Actor\n",
    "\n",
    "* Controller：负责 Serve 集群的控制循环，包括扩缩容、容灾、集群信息收集广播等等。\n",
    "* HTTPProxy：负责处理 HTTP 请求。\n",
    "* Replica：负责执行 Deployment 定义的逻辑。\n",
    "\n",
    "一般来说，用户只需要通过定义 Deployment 的逻辑，如下是一个用 gradio 做前端，运行 transfomers 模型的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe809d8-dd62-4389-8efc-4072c8e7d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from queue import Empty\n",
    "import threading\n",
    "from ray import serve\n",
    "from ray.serve.gradio_integrations import GradioIngress\n",
    "from transformers import TextIteratorStreamer, LlamaTokenizer, GenerationConfig, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430c8d0-315f-4703-af12-baf3edb931d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_MODEL_PATH = '/tmp-ms/model/alpaca-13B'\n",
    "\n",
    "@serve.deployment(ray_actor_options={'resources': {'NPU':1}})\n",
    "class GradioBot(GradioIngress):\n",
    "    def __init__(self):\n",
    "        import torch\n",
    "        import torch_npu\n",
    "        torch.npu.set_device(0)\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(LLAMA_MODEL_PATH, legacy=True)\n",
    "        self.model = LlamaForCasualLM.from_pretrained(LLAMA_MODEL_PATH, torch_dtype=torch.float16, device_map=torch.device('npu'))\n",
    "        super().__init__(self.get_gradio_app)\n",
    "\n",
    "    def get_gradio_app(self):\n",
    "        app = gr.ChatInterface(\n",
    "            self.query,\n",
    "            title='Llama Chat Bot'\n",
    "        ).queue()\n",
    "        return app\n",
    "\n",
    "    async def query(self, query, history):\n",
    "        temp = history\n",
    "\n",
    "        prompted_query = self.add_prompt(query)\n",
    "        streamer = TextIteratorStreamer(self.tokenizer, timeout=0, skip_prompt=True, skip_special_tokens=True)\n",
    "        generation_thread = threading.Thread(target=self.generate, kwargs=dict(prompted_query=prompted_query, streamer=streamer))\n",
    "        generation_thread.start()\n",
    "\n",
    "        output_text = ''\n",
    "        yield output_text\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                for token in streamer:\n",
    "                    output_text += token\n",
    "                    yiled output_text\n",
    "                break\n",
    "            except Empty:\n",
    "                await asyncio.sleep(0.01)\n",
    "\n",
    "    def add_prompt(self, instruction):\n",
    "        if isinstance(instruction, list):\n",
    "            return [self.add_prompt(item) for item in instruction]\n",
    "        return f\"[INST] <<SYS>>\\n You are a helpful assistant. 你是一个乐于助人的助手。\\n<<SYS>>\\n\\n{instruction} [/INST]\"\n",
    "\n",
    "    def generate(self, prompted_query, streamer: TextIteratorStreamer):\n",
    "        import torch\n",
    "        import torhc_npu\n",
    "        torch.npu.set_device(0)\n",
    "        torch_npu.npu.set_compile_mode(jit_compile=False)\n",
    "\n",
    "        inputs = self.tokenizer([prompted_query], return_returns='pt').to('npu')\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            num_beams=1,\n",
    "            repetition_penalty=1.1,\n",
    "            max_new_tokens=400\n",
    "        )\n",
    "        print()\n",
    "        self.model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            generation_config=generation_config,\n",
    "            streamer=streamer\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4e77f-7906-4d41-8f39-f64721117510",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradioBot = GradioBot.bind()\n",
    "serve.run(gradioBot, host='0.0.0.0', port=8001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
