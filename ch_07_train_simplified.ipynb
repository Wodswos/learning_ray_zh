{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35bd37d-ec6c-49d6-84c5-3dd5e8a8d7a4",
   "metadata": {},
   "source": [
    "Ray Train 最核心的 Trainer 类，Trainer 主要也是由两部分组成\n",
    "* ScalingConfig：定义物理进程实际资源实用\n",
    "* TrainingFunction：定义逻辑网络训练过程\n",
    "\n",
    "如下是一个用 TorchTrainer 训练的 demo，取自 Ray 官网：https://docs.ray.io/en/latest/train/getting-started-pytorch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e286c445-edcf-4ae3-aaf1-7ab6e4112788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-01-15 21:22:11</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:58.21        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.2/15.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 9.0/20 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_f470a_00000</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Five/ray_results/TorchTrainer_2024-01-15_21-21-07/TorchTrainer_f470a_00000_0_2024-01-15_21-21-13\\error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_f470a_00000</td><td>ERROR   </td><td>127.0.0.1:3744</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=37000)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(RayTrainWorker pid=37000)\u001b[0m [W socket.cpp:663] [c10d] The client socket has failed to connect to [huya.com]:3557 (system error: 10049 - 在其上下文中，该请求的地址无效。).\n",
      "\u001b[36m(TorchTrainer pid=3744)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=3744)\u001b[0m - (ip=127.0.0.1, pid=37000) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=3744)\u001b[0m - (ip=127.0.0.1, pid=30140) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=37000)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=37000)\u001b[0m Wrapping provided model in DistributedDataParallel.\n",
      "\u001b[36m(RayTrainWorker pid=30140)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=C:/Users/Five/ray_results/TorchTrainer_2024-01-15_21-21-07/TorchTrainer_f470a_00000_0_2024-01-15_21-21-13/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=30140)\u001b[0m [W socket.cpp:663] [c10d] The client socket has failed to connect to [huya.com]:3557 (system error: 10049 - 在其上下文中，该请求的地址无效。).\n",
      "2024-01-15 21:22:11,435\tERROR tune_controller.py:1374 -- Trial task failed for trial TorchTrainer_f470a_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(PermissionError): \u001b[36mray::_Inner.train()\u001b[39m (pid=3744, ip=127.0.0.1, actor_id=c46f591decb8399a1ea822fe01000000, repr=TorchTrainer)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 342, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\utils.py\", line 43, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\worker.py\", line 2624, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(PermissionError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=37000, ip=127.0.0.1, actor_id=9c2591b2903facf6a6dd48b501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x0000012B338956D0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1813, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\utils.py\", line 118, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Five\\AppData\\Local\\Temp\\ipykernel_32036\\3560008615.py\", line 55, in train_func\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\session.py\", line 644, in wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\session.py\", line 706, in report\n",
      "    _get_session().report(metrics, checkpoint=checkpoint)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\session.py\", line 417, in report\n",
      "    persisted_checkpoint = self.storage.persist_current_checkpoint(checkpoint)\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\storage.py\", line 558, in persist_current_checkpoint\n",
      "    _pyarrow_fs_copy_files(\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\storage.py\", line 110, in _pyarrow_fs_copy_files\n",
      "    return pyarrow.fs.copy_files(\n",
      "  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\pyarrow\\fs.py\", line 244, in copy_files\n",
      "    _copy_files_selector(source_fs, source_sel,\n",
      "  File \"pyarrow\\_fs.pyx\", line 1229, in pyarrow._fs._copy_files_selector\n",
      "  File \"pyarrow\\error.pxi\", line 110, in pyarrow.lib.check_status\n",
      "PermissionError: [WinError 32] Failed copying 'C:/Users/Five/AppData/Local/Temp/tmppdlqobme/model.pt' to 'C:/Users/Five/ray_results/TorchTrainer_2024-01-15_21-21-07/TorchTrainer_f470a_00000_0_2024-01-15_21-21-13/checkpoint_000000/model.pt'. Detail: [Windows error 32] 另一个程序正在使用此文件，进程无法访问。\n",
      "2024-01-15 21:22:11,473\tERROR tune.py:1038 -- Trials did not complete: [TorchTrainer_f470a_00000]\n",
      "2024-01-15 21:22:11,474\tINFO tune.py:1042 -- Total run time: 58.28 seconds (58.21 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"C:\\Users\\Five\\ray_results\\TorchTrainer_2024-01-15_21-21-07\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayTaskError(PermissionError)\u001b[0m             Traceback (most recent call last)",
      "\u001b[1;31mRayTaskError(PermissionError)\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=3744, ip=127.0.0.1, actor_id=c46f591decb8399a1ea822fe01000000, repr=TorchTrainer)\n  File \"python\\ray\\_raylet.pyx\", line 1813, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 342, in train\n    raise skipped from exception_cause(skipped)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\utils.py\", line 43, in check_for_failure\n    ray.get(object_ref)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 22, in auto_init_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\worker.py\", line 2624, in get\n    raise value.as_instanceof_cause()\nray.exceptions.RayTaskError(PermissionError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=37000, ip=127.0.0.1, actor_id=9c2591b2903facf6a6dd48b501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x0000012B338956D0>)\n  File \"python\\ray\\_raylet.pyx\", line 1813, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 1754, in ray._raylet.execute_task.function_executor\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 467, in _resume_span\n    return method(self, *_args, **_kwargs)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\worker_group.py\", line 33, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\utils.py\", line 118, in discard_return_wrapper\n    train_func(*args, **kwargs)\n  File \"C:\\Users\\Five\\AppData\\Local\\Temp\\ipykernel_32036\\3560008615.py\", line 55, in train_func\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\session.py\", line 644, in wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\session.py\", line 706, in report\n    _get_session().report(metrics, checkpoint=checkpoint)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\session.py\", line 417, in report\n    persisted_checkpoint = self.storage.persist_current_checkpoint(checkpoint)\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\storage.py\", line 558, in persist_current_checkpoint\n    _pyarrow_fs_copy_files(\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\_internal\\storage.py\", line 110, in _pyarrow_fs_copy_files\n    return pyarrow.fs.copy_files(\n  File \"C:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\pyarrow\\fs.py\", line 244, in copy_files\n    _copy_files_selector(source_fs, source_sel,\n  File \"pyarrow\\_fs.pyx\", line 1229, in pyarrow._fs._copy_files_selector\n  File \"pyarrow\\error.pxi\", line 110, in pyarrow.lib.check_status\nPermissionError: [WinError 32] Failed copying 'C:/Users/Five/AppData/Local/Temp/tmppdlqobme/model.pt' to 'C:/Users/Five/ray_results/TorchTrainer_2024-01-15_21-21-07/TorchTrainer_f470a_00000_0_2024-01-15_21-21-13/checkpoint_000000/model.pt'. Detail: [Windows error 32] 另一个程序正在使用此文件，进程无法访问。",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTrainingFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# [5] Launch distributed training job.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mtorch\u001b[38;5;241m.\u001b[39mTorchTrainer(\n\u001b[0;32m     74\u001b[0m     train_func,\n\u001b[0;32m     75\u001b[0m     scaling_config\u001b[38;5;241m=\u001b[39mscaling_config,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\u001b[39;00m\n\u001b[0;32m     81\u001b[0m )\n\u001b[1;32m---> 82\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# [6] Load the trained model.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m result\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mas_directory() \u001b[38;5;28;01mas\u001b[39;00m checkpoint_dir:\n",
      "File \u001b[1;32mC:\\repos\\Environment\\miniconda3\\envs\\ray\\lib\\site-packages\\ray\\train\\base_trainer.py:640\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    636\u001b[0m result \u001b[38;5;241m=\u001b[39m result_grid[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39merror:\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;66;03m# Raise trainable errors to the user with a message to restore\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;66;03m# or configure `FailureConfig` in a new run.\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError(\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([restore_msg, TrainingFailedError\u001b[38;5;241m.\u001b[39m_FAILURE_CONFIG_MSG])\n\u001b[0;32m    642\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresult\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merror\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTrainingFailedError\u001b[0m: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"C:\\Users\\Five\\ray_results\\TorchTrainer_2024-01-15_21-21-07\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries."
     ]
    }
   ],
   "source": [
    "# * 修改了每个 worker 的 GPU 数量，让他能在一个 gpu 上跑并行\n",
    "# * 修改了 backend，在 Windows 上不支持 NCCL\n",
    "#  不过 Windows 上还是会报错 PermissionError: [WinError 32] Failed copying 'C:/Users/xxx/model.pt' to 'C:/Users/yyy/model.pt' Detail: [Windows error 32] 另一个程序正在使用此文件，进程无法访问。\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "import ray.train.torch\n",
    "\n",
    "def train_func(config):\n",
    "    # Model, Loss, Optimizer\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    # model.to(\"cuda\")  # This is done by `prepare_model`\n",
    "    # [1] Prepare model.\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Data\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
    "    train_data = FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    # [2] Prepare dataloader.\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(10):\n",
    "        for images, labels in train_loader:\n",
    "            # This is done by `prepare_data_loader`!\n",
    "            # images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # [3] Report metrics and checkpoint.\n",
    "        metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics,\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            print(metrics)\n",
    "\n",
    "# [4] Configure scaling and resource requirements.\n",
    "scaling_config = ray.train.ScalingConfig(\n",
    "    num_workers=2, \n",
    "    resources_per_worker={\n",
    "        \"CPU\": 4,\n",
    "        \"GPU\": 0.5,\n",
    "    },\n",
    "    use_gpu=True\n",
    ")\n",
    "\n",
    "# [5] Launch distributed training job.\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    torch_config=ray.train.torch.TorchConfig(backend=\"gloo\")\n",
    "    # [5a] If running in a multi-node cluster, this is where you\n",
    "    # should configure the run's persistent storage that is accessible\n",
    "    # across all worker nodes.\n",
    "    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n",
    ")\n",
    "result = trainer.fit()\n",
    "\n",
    "# [6] Load the trained model.\n",
    "with result.checkpoint.as_directory() as checkpoint_dir:\n",
    "    model_state_dict = torch.load(os.path.join(checkpoint_dir, \"model.pt\"))\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc6d1f-a542-4e2d-acc1-3420af61c8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
